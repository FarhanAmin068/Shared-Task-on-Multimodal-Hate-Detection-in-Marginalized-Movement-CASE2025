{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-23T10:38:26.866841Z",
     "iopub.status.busy": "2025-07-23T10:38:26.866594Z",
     "iopub.status.idle": "2025-07-23T10:38:35.370666Z",
     "shell.execute_reply": "2025-07-23T10:38:35.369866Z",
     "shell.execute_reply.started": "2025-07-23T10:38:26.866818Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T10:38:35.372616Z",
     "iopub.status.busy": "2025-07-23T10:38:35.372300Z",
     "iopub.status.idle": "2025-07-23T10:39:35.877925Z",
     "shell.execute_reply": "2025-07-23T10:39:35.877333Z",
     "shell.execute_reply.started": "2025-07-23T10:38:35.372596Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, ViTModel\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import zipfile\n",
    "\n",
    "# Paths to datasets\n",
    "image_base_path = \"/kaggle/input/meme-dataset/meme_dataset/meme_dataset/Image_dataset\"\n",
    "train_csv = \"/kaggle/input/meme-dataset/meme_dataset/meme_dataset/Text_dataset/A_train.csv\"\n",
    "val_csv = \"/kaggle/input/meme-dataset/meme_dataset/meme_dataset/Text_dataset/A_val.csv\"\n",
    "test_csv = \"/kaggle/input/meme-dataset/meme_dataset/meme_dataset/Text_dataset/A_test.csv\"\n",
    "\n",
    "# Load datasets\n",
    "train_df = pd.read_csv(train_csv)\n",
    "val_df = pd.read_csv(val_csv)\n",
    "test_df = pd.read_csv(test_csv)\n",
    "\n",
    "# Construct image paths\n",
    "def get_train_image_path(row):\n",
    "    folder = \"Hate\" if row['label'] == 1 else \"No Hate\"\n",
    "    return os.path.join(image_base_path, f\"A_train_img/{folder}/{row['index']}\")\n",
    "\n",
    "train_df['image_path'] = train_df.apply(get_train_image_path, axis=1)\n",
    "val_df['image_path'] = val_df['index'].apply(lambda x: os.path.join(image_base_path, f\"A_val_img/{x}\"))\n",
    "test_df['image_path'] = test_df['index'].apply(lambda x: os.path.join(image_base_path, f\"A_test_img/{x}\"))\n",
    "\n",
    "# Check for missing images and remove them\n",
    "def filter_missing_images(df, name=\"dataset\"):\n",
    "    valid_rows = []\n",
    "    missing_count = 0\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=f\"Checking images in {name}\"):\n",
    "        if os.path.exists(row['image_path']):\n",
    "            valid_rows.append(row)\n",
    "        else:\n",
    "            missing_count += 1\n",
    "    print(f\"{name}: Removed {missing_count} samples with missing images\")\n",
    "    return pd.DataFrame(valid_rows).reset_index(drop=True)\n",
    "\n",
    "train_df = filter_missing_images(train_df, \"train\")\n",
    "val_df = filter_missing_images(val_df, \"val\")\n",
    "test_df = filter_missing_images(test_df, \"test\")\n",
    "\n",
    "# Text and image preprocessing\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"GroNLP/hateBERT\")\n",
    "\n",
    "def preprocess_text(text, max_length=128):\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return encoding['input_ids'].squeeze(), encoding['attention_mask'].squeeze()\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(224, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Custom dataset\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, df, transform, is_test=False):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        self.is_test = is_test\n",
    "        self.texts = df['text'].tolist()\n",
    "        self.image_paths = df['image_path'].tolist()\n",
    "        if not is_test:\n",
    "            self.labels = df['label'].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        input_ids, attention_mask = preprocess_text(text)\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        if self.is_test:\n",
    "            return {\n",
    "                'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask,\n",
    "                'image': image,\n",
    "                'index': self.df['index'].iloc[idx]\n",
    "            }\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'image': image,\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        }\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = MultimodalDataset(train_df, train_transform)\n",
    "val_dataset = MultimodalDataset(val_df, val_test_transform)\n",
    "test_dataset = MultimodalDataset(test_df, val_test_transform, is_test=True)\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "# Function to display images with text\n",
    "def display_samples(df, dataset_name, num_samples=5, is_test=False):\n",
    "    samples = df.head(min(num_samples, len(df)))\n",
    "    fig, axes = plt.subplots(1, num_samples, figsize=(5 * num_samples, 5))\n",
    "    if num_samples == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, (_, row) in enumerate(samples.iterrows()):\n",
    "        image_path = row['image_path']\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        axes[i].imshow(image)\n",
    "        axes[i].axis('off')\n",
    "        text = row['text']\n",
    "        if is_test:\n",
    "            title = f\"Index: {row['index']}\\nText: {text}\"\n",
    "        else:\n",
    "            label = \"Hate\" if row['label'] == 1 else \"No Hate\"\n",
    "            title = f\"Index: {row['index']}\\nText: {text}\\nLabel: {label}\"\n",
    "        wrapped_text = \"\\n\".join([text[j:j+50] for j in range(0, len(text), 50)])\n",
    "        title = title.replace(text, wrapped_text)\n",
    "        axes[i].set_title(title, fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Display samples from each dataset\n",
    "print(\"Displaying Train Samples:\")\n",
    "display_samples(train_df, \"train\")\n",
    "print(\"\\nDisplaying Validation Samples:\")\n",
    "display_samples(val_df, \"val\")\n",
    "print(\"\\nDisplaying Test Samples:\")\n",
    "display_samples(test_df, \"test\", is_test=True)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=2)\n",
    "\n",
    "# Multimodal model with late fusion\n",
    "class MultimodalModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultimodalModel, self).__init__()\n",
    "        self.text_model = AutoModel.from_pretrained(\"GroNLP/hateBERT\")\n",
    "        self.image_model = ViTModel.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "        self.fc = nn.Linear(768 * 2, 1)  # Late fusion: Concatenate 768-dim text and image embeddings\n",
    "        self.dropout = nn.Dropout(0.3)  # Dropout for regularization\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, image):\n",
    "        # Text embeddings\n",
    "        text_outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_embeds = text_outputs.pooler_output  # [batch_size, 768]\n",
    "        \n",
    "        # Image embeddings\n",
    "        image_outputs = self.image_model(pixel_values=image)\n",
    "        image_embeds = image_outputs.pooler_output  # [batch_size, 768]\n",
    "        \n",
    "        # Late fusion: Concatenate text and image embeddings\n",
    "        combined = torch.cat((text_embeds, image_embeds), dim=1)  # [batch_size, 768*2]\n",
    "        combined = self.dropout(combined)  # Apply dropout\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.fc(combined)  # [batch_size, 1]\n",
    "        return logits\n",
    "\n",
    "# Initialize model, optimizer, and loss\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MultimodalModel().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T10:39:35.878943Z",
     "iopub.status.busy": "2025-07-23T10:39:35.878685Z",
     "iopub.status.idle": "2025-07-23T11:17:21.358494Z",
     "shell.execute_reply": "2025-07-23T11:17:21.357754Z",
     "shell.execute_reply.started": "2025-07-23T10:39:35.878923Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(loader, desc=\"Training\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        images = batch['image'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask, images).squeeze()\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    preds, true_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            images = batch['image'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask, images).squeeze()\n",
    "            preds.extend(torch.sigmoid(logits).cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    preds = [1 if p >= 0.5 else 0 for p in preds]\n",
    "    accuracy = accuracy_score(true_labels, preds)\n",
    "    f1 = f1_score(true_labels, preds, zero_division=0)\n",
    "    precision = precision_score(true_labels, preds, zero_division=0)\n",
    "    recall = recall_score(true_labels, preds, zero_division=0)\n",
    "    return accuracy, f1, precision, recall\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "best_val_acc = 0\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, criterion)\n",
    "    val_acc, val_f1, val_precision, val_recall = evaluate(model, val_loader)\n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, \"\n",
    "          f\"Val Accuracy = {val_acc:.4f}, Val F1 = {val_f1:.4f}, \"\n",
    "          f\"Val Precision = {val_precision:.4f}, Val Recall = {val_recall:.4f}\")\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T11:17:21.360342Z",
     "iopub.status.busy": "2025-07-23T11:17:21.359994Z",
     "iopub.status.idle": "2025-07-23T11:17:37.132304Z",
     "shell.execute_reply": "2025-07-23T11:17:37.131283Z",
     "shell.execute_reply.started": "2025-07-23T11:17:21.360292Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Test predictions\n",
    "model.eval()\n",
    "test_preds = []\n",
    "test_indices = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Predicting\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        images = batch['image'].to(device)\n",
    "        indices = batch['index']\n",
    "        \n",
    "        logits = model(input_ids, attention_mask, images).squeeze()\n",
    "        preds = torch.sigmoid(logits).cpu().numpy()\n",
    "        preds = [1 if p >= 0.5 else 0 for p in preds]\n",
    "        test_preds.extend(preds)\n",
    "        test_indices.extend(indices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-23T11:17:37.134204Z",
     "iopub.status.busy": "2025-07-23T11:17:37.133931Z",
     "iopub.status.idle": "2025-07-23T11:17:37.145116Z",
     "shell.execute_reply": "2025-07-23T11:17:37.144267Z",
     "shell.execute_reply.started": "2025-07-23T11:17:37.134176Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create JSON Lines submission\n",
    "submission_data = [{\"index\": f\"{idx}.png\", \"prediction\": pred} for idx, pred in zip(test_indices, test_preds)]\n",
    "submission_data = sorted(submission_data, key=lambda x: int(x['index'].replace('.png', '')))\n",
    "with open(\"submission.json\", \"w\") as f:\n",
    "    for entry in submission_data:\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n",
    "print(\"Test predictions saved to submission.json\")\n",
    "\n",
    "# Create ref.zip with submission.json\n",
    "with zipfile.ZipFile(\"ref.zip\", \"w\", zipfile.ZIP_DEFLATED) as zf:\n",
    "    zf.write(\"submission.json\")\n",
    "print(\"Zipped submission.json into ref.zip\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7769003,
     "sourceId": 12397400,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
