{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-11T20:08:29.116847Z",
     "iopub.status.busy": "2025-07-11T20:08:29.116268Z",
     "iopub.status.idle": "2025-07-11T20:08:36.346444Z",
     "shell.execute_reply": "2025-07-11T20:08:36.345722Z",
     "shell.execute_reply.started": "2025-07-11T20:08:29.116822Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T20:08:36.347933Z",
     "iopub.status.busy": "2025-07-11T20:08:36.347726Z",
     "iopub.status.idle": "2025-07-11T20:08:59.975627Z",
     "shell.execute_reply": "2025-07-11T20:08:59.975055Z",
     "shell.execute_reply.started": "2025-07-11T20:08:36.347917Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import AutoImageProcessor, SwinModel, DistilBertTokenizer, DistilBertModel\n",
    "from torch.optim import AdamW\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, mean_squared_error\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T20:08:59.976790Z",
     "iopub.status.busy": "2025-07-11T20:08:59.976251Z",
     "iopub.status.idle": "2025-07-11T20:08:59.980675Z",
     "shell.execute_reply": "2025-07-11T20:08:59.979930Z",
     "shell.execute_reply.started": "2025-07-11T20:08:59.976769Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set base paths\n",
    "base_text_dir = '/kaggle/input/meme-dataset/meme_dataset/meme_dataset/Text_dataset/'\n",
    "base_image_dir = '/kaggle/input/meme-dataset/meme_dataset/meme_dataset/Image_dataset/'\n",
    "output_dir = '/kaggle/working/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T20:08:59.982144Z",
     "iopub.status.busy": "2025-07-11T20:08:59.981941Z",
     "iopub.status.idle": "2025-07-11T20:08:59.999545Z",
     "shell.execute_reply": "2025-07-11T20:08:59.999012Z",
     "shell.execute_reply.started": "2025-07-11T20:08:59.982128Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Paths for text and image datasets\n",
    "train_csv = os.path.join(base_text_dir, 'A_train.csv')\n",
    "val_csv = os.path.join(base_text_dir, 'A_val.csv')\n",
    "test_csv = os.path.join(base_text_dir, 'A_test.csv')\n",
    "train_img_dir = os.path.join(base_image_dir, 'A_train_img')\n",
    "val_img_dir = os.path.join(base_image_dir, 'A_val_img')\n",
    "test_img_dir = os.path.join(base_image_dir, 'A_test_img')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T20:09:00.000209Z",
     "iopub.status.busy": "2025-07-11T20:09:00.000055Z",
     "iopub.status.idle": "2025-07-11T20:09:00.016664Z",
     "shell.execute_reply": "2025-07-11T20:09:00.016109Z",
     "shell.execute_reply.started": "2025-07-11T20:09:00.000196Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from functools import wraps\n",
    "\n",
    "# NaN-safe decorator\n",
    "def skip_if_nan(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(text):\n",
    "        if pd.isna(text):\n",
    "            return text\n",
    "        return func(text)\n",
    "    return wrapper\n",
    "\n",
    "# Precompiled patterns\n",
    "url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "html_pattern = re.compile(r'<.*?>')\n",
    "emoji_pattern = re.compile(\n",
    "    \"[\" \n",
    "    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "    u\"\\U0001F680-\\U0001F6FF\"  # transport & map\n",
    "    u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
    "    u\"\\U00002702-\\U000027B0\"\n",
    "    u\"\\U000024C2-\\U0001F251\"\n",
    "    \"]+\", flags=re.UNICODE)\n",
    "\n",
    "@skip_if_nan\n",
    "def to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "@skip_if_nan\n",
    "def remove_urls(text):\n",
    "    return url_pattern.sub('', text)\n",
    "\n",
    "@skip_if_nan\n",
    "def remove_html(text):\n",
    "    return html_pattern.sub('', text)\n",
    "\n",
    "@skip_if_nan\n",
    "def remove_emojis(text):\n",
    "    return emoji_pattern.sub('', text)\n",
    "\n",
    "@skip_if_nan\n",
    "def remove_extra_whitespace(text):\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "@skip_if_nan\n",
    "def clean_text(text):\n",
    "    text = remove_urls(text)\n",
    "    text = remove_html(text)\n",
    "    text = remove_emojis(text)\n",
    "    text = to_lowercase(text)  # only if using an uncased model\n",
    "    text = remove_extra_whitespace(text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T20:09:00.017677Z",
     "iopub.status.busy": "2025-07-11T20:09:00.017423Z",
     "iopub.status.idle": "2025-07-11T20:09:07.954369Z",
     "shell.execute_reply": "2025-07-11T20:09:07.953525Z",
     "shell.execute_reply.started": "2025-07-11T20:09:00.017662Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Function to map image paths, using index as filename (e.g., \"1002.png\")\n",
    "def get_image_path(index, img_dir, is_train=False):\n",
    "    if is_train:\n",
    "        # Training set: check Hate and No Hate subfolders\n",
    "        subfolders = ['Hate', 'No Hate', 'hate', 'no_hate', 'No_Hate']\n",
    "        for subfolder in subfolders:\n",
    "            img_path = os.path.join(img_dir, subfolder, index)\n",
    "            if os.path.exists(img_path):\n",
    "                print(f\"Found image: {img_path}\")\n",
    "                return img_path\n",
    "        print(f\"No image found for index {index} in {img_dir}\")\n",
    "        return None\n",
    "    else:\n",
    "        # Validation and test sets: check directly in the folder\n",
    "        img_path = os.path.join(img_dir, index)\n",
    "        if os.path.exists(img_path):\n",
    "            print(f\"Found image: {img_path}\")\n",
    "            return img_path\n",
    "        print(f\"No image found for index {index} in {img_dir}\")\n",
    "        return None\n",
    "\n",
    "# Load text data\n",
    "print(\"Loading CSV files...\")\n",
    "try:\n",
    "    train_df = pd.read_csv(train_csv)\n",
    "    val_df = pd.read_csv(val_csv)\n",
    "    test_df = pd.read_csv(test_csv)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading CSV files: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"Train CSV head:\")\n",
    "print(train_df.head())\n",
    "print(\"Validation CSV head:\")\n",
    "print(val_df.head())\n",
    "print(\"Test CSV head:\")\n",
    "print(test_df.head())\n",
    "\n",
    "# Verify index column\n",
    "if 'index' not in train_df.columns or 'index' not in val_df.columns or 'index' not in test_df.columns:\n",
    "    print(\"Error: 'index' column missing in one or more CSV files\")\n",
    "    raise ValueError(\"Missing 'index' column\")\n",
    "\n",
    "# Clean text column\n",
    "print(\"Cleaning text data...\")\n",
    "train_df['text'] = train_df['text'].astype(str).apply(clean_text)\n",
    "val_df['text'] = val_df['text'].astype(str).apply(clean_text)\n",
    "test_df['text'] = test_df['text'].astype(str).apply(clean_text)\n",
    "\n",
    "# Map labels (already 0 or 1)\n",
    "train_df['label'] = train_df['label']\n",
    "val_df['label'] = val_df['label']\n",
    "\n",
    "# Add image paths\n",
    "print(\"Mapping image paths...\")\n",
    "train_df['image_path'] = train_df['index'].apply(lambda x: get_image_path(x, train_img_dir, is_train=True))\n",
    "val_df['image_path'] = val_df['index'].apply(lambda x: get_image_path(x, val_img_dir, is_train=False))\n",
    "test_df['image_path'] = test_df['index'].apply(lambda x: get_image_path(x, test_img_dir, is_train=False))\n",
    "\n",
    "# Log the number of matched images\n",
    "print(f\"Train: {train_df['image_path'].notna().sum()} images matched out of {len(train_df)}\")\n",
    "print(f\"Validation: {val_df['image_path'].notna().sum()} images matched out of {len(val_df)}\")\n",
    "print(f\"Test: {test_df['image_path'].notna().sum()} images matched out of {len(test_df)}\")\n",
    "\n",
    "# Filter out rows with missing images\n",
    "train_df = train_df[train_df['image_path'].notna()]\n",
    "val_df = val_df[val_df['image_path'].notna()]\n",
    "test_df = test_df[test_df['image_path'].notna()]\n",
    "\n",
    "# Check if DataFrames are empty\n",
    "if train_df.empty:\n",
    "    print(\"Error: Train DataFrame is empty after filtering\")\n",
    "if val_df.empty:\n",
    "    print(\"Error: Validation DataFrame is empty after filtering\")\n",
    "if test_df.empty:\n",
    "    print(\"Error: Test DataFrame is empty after filtering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T20:09:07.955761Z",
     "iopub.status.busy": "2025-07-11T20:09:07.955394Z",
     "iopub.status.idle": "2025-07-11T20:09:08.015312Z",
     "shell.execute_reply": "2025-07-11T20:09:08.014626Z",
     "shell.execute_reply.started": "2025-07-11T20:09:07.955732Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save preprocessed data\n",
    "train_df.to_csv(os.path.join(output_dir, 'train_processed1.csv'), index=False)\n",
    "val_df.to_csv(os.path.join(output_dir, 'val_processed1.csv'), index=False)\n",
    "test_df.to_csv(os.path.join(output_dir, 'test_processed1.csv'), index=False)\n",
    "\n",
    "print(\"Preprocessed data saved:\")\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Validation shape: {val_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T20:09:08.016996Z",
     "iopub.status.busy": "2025-07-11T20:09:08.016147Z",
     "iopub.status.idle": "2025-07-11T20:09:08.029747Z",
     "shell.execute_reply": "2025-07-11T20:09:08.029189Z",
     "shell.execute_reply.started": "2025-07-11T20:09:08.016965Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "def show_sample(df, num_samples=5, title=\"Sample Data\"):\n",
    "    sample_df = df.sample(n=min(num_samples, len(df)))\n",
    "\n",
    "    plt.figure(figsize=(15, 3 * num_samples))\n",
    "    for i, (_, row) in enumerate(sample_df.iterrows()):\n",
    "        try:\n",
    "            img = Image.open(row['image_path']).convert(\"RGB\")\n",
    "\n",
    "            plt.subplot(num_samples, 1, i + 1)\n",
    "            plt.imshow(img)\n",
    "            plt.axis('off')\n",
    "\n",
    "            label_text = f\"Label: {row['label']} | \" if 'label' in row else \"\"\n",
    "            plt.title(f\"{label_text}Text: {row['text']}\", fontsize=12)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load image: {row['image_path']} â€” {e}\")\n",
    "\n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T20:09:08.031206Z",
     "iopub.status.busy": "2025-07-11T20:09:08.030502Z",
     "iopub.status.idle": "2025-07-11T20:09:08.781131Z",
     "shell.execute_reply": "2025-07-11T20:09:08.780463Z",
     "shell.execute_reply.started": "2025-07-11T20:09:08.031187Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "show_sample(train_df, num_samples=1, title=\"Train Samples\")\n",
    "show_sample(val_df, num_samples=1, title=\"Validation Samples\")\n",
    "show_sample(test_df, num_samples=1, title=\"Test Samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T20:09:08.783029Z",
     "iopub.status.busy": "2025-07-11T20:09:08.782826Z",
     "iopub.status.idle": "2025-07-11T20:33:51.466269Z",
     "shell.execute_reply": "2025-07-11T20:33:51.465315Z",
     "shell.execute_reply.started": "2025-07-11T20:09:08.783015Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Custom Dataset class for multimodal data\n",
    "class MemeDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_processor, tokenizer, max_length=128):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_processor = image_processor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        # Load and process image\n",
    "        try:\n",
    "            image = Image.open(row['image_path']).convert('RGB')\n",
    "            image = self.transform(image)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {row['image_path']}: {e}\")\n",
    "            image = torch.zeros(3, 224, 224)  # Fallback empty image\n",
    "\n",
    "        # Process text\n",
    "        text = row['text']\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        item = {\n",
    "            'image': image,\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'index': row['index']  # Include index\n",
    "        }\n",
    "\n",
    "        # Include label for train/val, exclude for test\n",
    "        if 'label' in row:\n",
    "            item['label'] = torch.tensor(row['label'], dtype=torch.float)\n",
    "\n",
    "        return item\n",
    "\n",
    "class MultimodalMemeClassifier(nn.Module):\n",
    "    def __init__(self, swin_model_name=\"microsoft/swin-tiny-patch4-window7-224\", \n",
    "                 distilbert_model_name=\"distilbert-base-uncased\", hidden_dim=256):\n",
    "        super(MultimodalMemeClassifier, self).__init__()\n",
    "        # Image model (Swin Transformer)\n",
    "        self.swin = SwinModel.from_pretrained(swin_model_name)\n",
    "        \n",
    "        # Text model (DistilBERT)\n",
    "        self.distilbert = DistilBertModel.from_pretrained(distilbert_model_name)\n",
    "        \n",
    "        # Separate branches for late-fusion\n",
    "        swin_output_dim = self.swin.config.hidden_size\n",
    "        distilbert_output_dim = self.distilbert.config.hidden_size\n",
    "        \n",
    "        # Fully connected layers for each modality\n",
    "        self.image_fc = nn.Linear(swin_output_dim, hidden_dim)\n",
    "        self.text_fc = nn.Linear(distilbert_output_dim, hidden_dim)\n",
    "        \n",
    "        # Fusion and classification layers\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(hidden_dim * 2, 1)  # Combine features from both modalities\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        # Image features\n",
    "        swin_outputs = self.swin(pixel_values=images)\n",
    "        image_features = swin_outputs.pooler_output  # Use pooler_output (batch_size, hidden_size)\n",
    "        if image_features is None:\n",
    "            image_features = swin_outputs.last_hidden_state.mean(dim=1)  # Fallback to mean pooling\n",
    "        \n",
    "        # Process image features through separate FC layer\n",
    "        image_features = F.relu(self.image_fc(image_features))\n",
    "        \n",
    "        # Text features\n",
    "        text_outputs = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_features = text_outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "        \n",
    "        # Process text features through separate FC layer\n",
    "        text_features = F.relu(self.text_fc(text_features))\n",
    "        \n",
    "        # Combine features (late-fusion)\n",
    "        combined = torch.cat((image_features, text_features), dim=1)\n",
    "        combined = self.dropout(combined)\n",
    "        logits = self.classifier(combined)\n",
    "        probs = self.sigmoid(logits)\n",
    "        return probs\n",
    "        \n",
    "# Initialize processors and tokenizers\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = MemeDataset(train_df, image_processor, tokenizer)\n",
    "val_dataset = MemeDataset(val_df, image_processor, tokenizer)\n",
    "test_dataset = MemeDataset(test_df, image_processor, tokenizer)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# Initialize model, optimizer, and loss\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MultimodalMemeClassifier().to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Training function\n",
    "def train_epoch(model, data_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    for batch in tqdm(data_loader, desc=\"Training\"):\n",
    "        images = batch['image'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images, input_ids, attention_mask).squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        preds = (outputs >= 0.5).float()\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary')\n",
    "    return avg_loss, accuracy, precision, recall, f1\n",
    "\n",
    "# Validation function\n",
    "def evaluate(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            images = batch['image'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(images, input_ids, attention_mask).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            preds = (outputs >= 0.5).float()\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary')\n",
    "    return avg_loss, accuracy, precision, recall, f1\n",
    "\n",
    "# Test prediction function\n",
    "def predict_test(model, data_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_indices = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Predicting\"):\n",
    "            images = batch['image'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            \n",
    "            outputs = model(images, input_ids, attention_mask).squeeze()\n",
    "            preds = (outputs >= 0.5).float()\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_indices.extend(batch['index'])\n",
    "            \n",
    "    return all_preds, all_indices\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 15\n",
    "best_val_f1 = 0\n",
    "best_model_path = os.path.join(output_dir, 'best_model.pth')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc, train_prec, train_rec, train_f1 = train_epoch(\n",
    "        model, train_loader, criterion, optimizer, device\n",
    "    )\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, \"\n",
    "          f\"Prec: {train_prec:.4f}, Rec: {train_rec:.4f}, F1: {train_f1:.4f}\")\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc, val_prec, val_rec, val_f1 = evaluate(\n",
    "        model, val_loader, criterion, device\n",
    "    )\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, \"\n",
    "          f\"Prec: {val_prec:.4f}, Rec: {val_rec:.4f}, F1: {val_f1:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"Saved best model with F1: {best_val_f1:.4f}\")\n",
    "\n",
    "# Load best model for testing\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "print(\"Loaded best model for testing\")\n",
    "\n",
    "# Generate test predictions\n",
    "test_preds, test_indices = predict_test(model, test_loader, device)\n",
    "\n",
    "# Save test predictions\n",
    "test_results = pd.DataFrame({\n",
    "    'index': test_indices,\n",
    "    'prediction': test_preds\n",
    "})\n",
    "test_results.to_csv(os.path.join(output_dir, 'test_predictions.csv'), index=False)\n",
    "print(f\"Test predictions saved to {os.path.join(output_dir, 'test_predictions.csv')}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7769003,
     "sourceId": 12397400,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
